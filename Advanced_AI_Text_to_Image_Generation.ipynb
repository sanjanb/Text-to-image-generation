{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sanjanb/Text-to-image-generation/blob/main/Advanced_AI_Text_to_Image_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text to Image Generation"
      ],
      "metadata": {
        "id": "npq4ZclU-RC2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lIYdn1woOS1n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3235f1ca-c349-4c7c-86d6-4cb0ee4f9d3b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.8/126.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m507.1/507.1 kB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.2/8.2 MB\u001b[0m \u001b[31m48.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m265.7/265.7 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m36.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "dask-expr 1.1.21 requires pyarrow>=14.0.1, but you have pyarrow 14.0.0 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2023.10.0 which is incompatible.\n",
            "sentence-transformers 4.1.0 requires transformers<5.0.0,>=4.41.0, but you have transformers 4.36.0 which is incompatible.\n",
            "bigframes 2.4.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting diffusers==0.25.0 (from diffusers[training]==0.25.0)\n",
            "  Downloading diffusers-0.25.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (0.31.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0->diffusers[training]==0.25.0) (11.2.1)\n",
            "Requirement already satisfied: accelerate>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from diffusers[training]==0.25.0) (0.25.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (from diffusers[training]==0.25.0) (2.16.0)\n",
            "Collecting protobuf<4,>=3.20.3 (from diffusers[training]==0.25.0)\n",
            "  Downloading protobuf-3.20.3-py2.py3-none-any.whl.metadata (720 bytes)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.11/dist-packages (from diffusers[training]==0.25.0) (2.18.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.11/dist-packages (from diffusers[training]==0.25.0) (3.1.6)\n",
            "Requirement already satisfied: peft>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from diffusers[training]==0.25.0) (0.15.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.11.0->diffusers[training]==0.25.0) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.11.0->diffusers[training]==0.25.0) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.11.0->diffusers[training]==0.25.0) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from accelerate>=0.11.0->diffusers[training]==0.25.0) (2.6.0+cu124)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0->diffusers[training]==0.25.0) (2023.10.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0->diffusers[training]==0.25.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0->diffusers[training]==0.25.0) (4.13.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft>=0.6.0->diffusers[training]==0.25.0) (4.36.0)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (14.0.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (0.7)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (0.70.15)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->diffusers[training]==0.25.0) (3.11.15)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0->diffusers[training]==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0->diffusers[training]==0.25.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0->diffusers[training]==0.25.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0->diffusers[training]==0.25.0) (2025.4.26)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.25.0->diffusers[training]==0.25.0) (3.21.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2->diffusers[training]==0.25.0) (3.0.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (1.71.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (3.8)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard->diffusers[training]==0.25.0) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->diffusers[training]==0.25.0) (1.20.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.10.0->accelerate>=0.11.0->diffusers[training]==0.25.0) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->diffusers[training]==0.25.0) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->diffusers[training]==0.25.0) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->diffusers[training]==0.25.0) (2025.2)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.11/dist-packages (from transformers->peft>=0.6.0->diffusers[training]==0.25.0) (0.15.2)\n",
            "Downloading diffusers-0.25.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-3.20.3-py2.py3-none-any.whl (162 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m162.1/162.1 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf, diffusers\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.4\n",
            "    Uninstalling protobuf-5.29.4:\n",
            "      Successfully uninstalled protobuf-5.29.4\n",
            "  Attempting uninstall: diffusers\n",
            "    Found existing installation: diffusers 0.33.1\n",
            "    Uninstalling diffusers-0.33.1:\n",
            "      Successfully uninstalled diffusers-0.33.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow-metadata 1.17.1 requires protobuf<6.0.0,>=4.25.2; python_version >= \"3.11\", but you have protobuf 3.20.3 which is incompatible.\n",
            "bigframes 2.4.0 requires pyarrow>=15.0.2, but you have pyarrow 14.0.0 which is incompatible.\n",
            "ydf 0.11.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 3.20.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 3.20.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed diffusers-0.25.0 protobuf-3.20.3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "01a37edd77a44dc5ace56adaa17e60b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: diffusers==0.25.0 in /usr/local/lib/python3.11/dist-packages (0.25.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (8.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (0.31.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (0.5.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers==0.25.0) (11.2.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0) (2023.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.19.4->diffusers==0.25.0) (4.13.2)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers==0.25.0) (3.21.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->diffusers==0.25.0) (2025.4.26)\n"
          ]
        }
      ],
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_id = \"google/ddpm-celebahq-256\"\n",
        "pipeline = DDPMPipeline.from_pretrained(model_id)\n",
        "pipeline.to(device)\n",
        "\n",
        "generated_image = pipeline().images[0]\n",
        "generated_image\n"
      ],
      "metadata": {
        "id": "JRTOOjfzomEK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "08f54e60-5ed2-40bd-ed64-560385cc7e33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-0b98f69b8bce>\", line 3, in <cell line: 0>\n",
            "    from datasets import load_dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\", line 22, in <module>\n",
            "    from .arrow_dataset import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 59, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 26, in <module>\n",
            "    from pandas.compat import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/__init__.py\", line 27, in <module>\n",
            "    from pandas.compat.pyarrow import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/compat/pyarrow.py\", line 8, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-0b98f69b8bce>\", line 3, in <cell line: 0>\n",
            "    from datasets import load_dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\", line 22, in <module>\n",
            "    from .arrow_dataset import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 59, in <module>\n",
            "    import pandas as pd\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/__init__.py\", line 49, in <module>\n",
            "    from pandas.core.api import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/api.py\", line 9, in <module>\n",
            "    from pandas.core.dtypes.dtypes import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pandas/core/dtypes/dtypes.py\", line 24, in <module>\n",
            "    from pandas._libs import (\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "A module that was compiled using NumPy 1.x cannot be run in\n",
            "NumPy 2.0.2 as it may crash. To support both 1.x and 2.x\n",
            "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
            "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
            "\n",
            "If you are a user of the module, the easiest solution will be to\n",
            "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
            "We expect that some modules will need time to support NumPy 2.\n",
            "\n",
            "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/colab_kernel_launcher.py\", line 37, in <module>\n",
            "    ColabKernelApp.launch_instance()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/traitlets/config/application.py\", line 992, in launch_instance\n",
            "    app.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelapp.py\", line 712, in start\n",
            "    self.io_loop.start()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/tornado/platform/asyncio.py\", line 205, in start\n",
            "    self.asyncio_loop.run_forever()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 608, in run_forever\n",
            "    self._run_once()\n",
            "  File \"/usr/lib/python3.11/asyncio/base_events.py\", line 1936, in _run_once\n",
            "    handle._run()\n",
            "  File \"/usr/lib/python3.11/asyncio/events.py\", line 84, in _run\n",
            "    self._context.run(self._callback, *self._args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 510, in dispatch_queue\n",
            "    await self.process_one()\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 499, in process_one\n",
            "    await dispatch(*args)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 406, in dispatch_shell\n",
            "    await result\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\", line 730, in execute_request\n",
            "    reply_content = await reply_content\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/ipkernel.py\", line 383, in do_execute\n",
            "    res = shell.run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/ipykernel/zmqshell.py\", line 528, in run_cell\n",
            "    return super().run_cell(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2975, in run_cell\n",
            "    result = self._run_cell(\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3030, in _run_cell\n",
            "    return runner(coro)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/async_helpers.py\", line 78, in _pseudo_sync_runner\n",
            "    coro.send(None)\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3257, in run_cell_async\n",
            "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3473, in run_ast_nodes\n",
            "    if (await self.run_code(code, result,  async_=asy)):\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-2-0b98f69b8bce>\", line 3, in <cell line: 0>\n",
            "    from datasets import load_dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\", line 22, in <module>\n",
            "    from .arrow_dataset import Dataset\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\", line 60, in <module>\n",
            "    import pyarrow as pa\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\", line 65, in <module>\n",
            "    import pyarrow.lib as _lib\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "_ARRAY_API not found",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
          ]
        },
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "numpy.core.multiarray failed to import",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-0b98f69b8bce>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorchvision\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtransforms\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"2.16.0\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_dataset\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCommitInfo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCommitOperationAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCommitOperationDelete\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetCard\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetCardData\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0m_gc_enabled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misenabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyarrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m_gc_enabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0m_gc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pyarrow/lib.pyx\u001b[0m in \u001b[0;36minit pyarrow.lib\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: numpy.core.multiarray failed to import",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "img = torchvision.transforms.ToTensor()(generated_image)\n",
        "type(img)"
      ],
      "metadata": {
        "id": "gWyQjgO3ppoq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img.shape"
      ],
      "metadata": {
        "id": "io5mfOiOsxKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JHjmWHImYHtp"
      },
      "outputs": [],
      "source": [
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "model = UNet2DModel.from_pretrained(model_id)\n",
        "scheduler = DDIMScheduler.from_pretrained(model_id)\n",
        "scheduler.set_timesteps(num_inference_steps=50)\n",
        "scheduler.timesteps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.randn?"
      ],
      "metadata": {
        "id": "N212Xbatsd0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1)\n",
        "\n",
        "image = torch.randn(\n",
        "    1, model.config.in_channels, model.config.sample_size, model.config.sample_size\n",
        ")\n",
        "image.shape"
      ],
      "metadata": {
        "id": "hlkaoYEpsRny"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(image)"
      ],
      "metadata": {
        "id": "DkfuwKmEtcAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    noise_prediction = model(sample=image, timestep=980).sample\n",
        "\n",
        "noise_prediction.shape"
      ],
      "metadata": {
        "id": "iOJKecXAs74W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler.step?"
      ],
      "metadata": {
        "id": "HyR-m41dstyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L2pMAbMyYu69"
      },
      "outputs": [],
      "source": [
        "scheduler_output = scheduler.step(\n",
        "    model_output=noise_prediction, timestep=960, sample=image\n",
        ")\n",
        "\n",
        "image = scheduler_output.prev_sample\n",
        "\n",
        "image.shape"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "show_images(image)"
      ],
      "metadata": {
        "id": "uQznq9PMI8Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSRmQd9_YL_V"
      },
      "outputs": [],
      "source": [
        "#if we combine all of this, and this time just to save time, using a diffusion model with 50 steps\n",
        "\n",
        "import tqdm\n",
        "import PIL\n",
        "\n",
        "model = UNet2DModel.from_pretrained(model_id)\n",
        "scheduler = DDIMScheduler.from_pretrained(model_id)\n",
        "scheduler.set_timesteps(num_inference_steps=50)\n",
        "scheduler.timesteps\n",
        "\n",
        "# We are starting off with a batch on just one image\n",
        "image = torch.randn(\n",
        "    1, model.config.in_channels, model.config.sample_size, model.config.sample_size\n",
        ")\n",
        "\n",
        "model.to(\"cuda\")\n",
        "image = image.to(\"cuda\")\n",
        "\n",
        "for index, timestep in enumerate(tqdm.tqdm(scheduler.timesteps)):\n",
        "  with torch.no_grad():\n",
        "      noise_prediction = model(sample=image, timestep=timestep).sample\n",
        "\n",
        "  scheduler_output = scheduler.step(\n",
        "      model_output=noise_prediction, timestep=timestep, sample=image\n",
        "  )\n",
        "\n",
        "  image = scheduler_output.prev_sample\n",
        "\n",
        "  if (index + 1) % 10 == 0:\n",
        "      display_sample(image, index + 1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "je02XoCmihe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHizgi4YyKad"
      },
      "source": [
        "## Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "F9130cxS-4qK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sgs7bFgZxGV7"
      },
      "outputs": [],
      "source": [
        "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
        "\n",
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),  # Resize\n",
        "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
        "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "batch = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
        "print(\"X shape:\", batch.shape)\n",
        "show_images(batch).resize((8 * 64, 64), resample=Image.NEAREST)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DDPMScheduler?"
      ],
      "metadata": {
        "id": "dl-7bbApB-mO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xRMLVfl5xXs-"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "noise_scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EiyakvzfyVlX"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.005)\n",
        "original_image = noise_scheduler.alphas_cumprod.cpu() ** 0.5\n",
        "noise = (1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5\n",
        "\n",
        "plt.title(\"Low values for beta_end\")\n",
        "plt.plot(original_image, label=\"Component of original image\")\n",
        "plt.plot(noise, label=\"Component of noise\")\n",
        "plt.legend(fontsize=\"medium\", loc='center right')\n",
        "\n",
        "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
        "noise = torch.randn_like(batch)\n",
        "noisy_batch = noise_scheduler.add_noise(batch, noise, timesteps)\n",
        "print(\"Noisy X shape\", noisy_batch.shape)\n",
        "show_images(noisy_batch).resize((8 * 64, 64), resample=Image.NEAREST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Aal7KF1rner"
      },
      "outputs": [],
      "source": [
        "#High values for beta_end\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_start=0.001, beta_end=0.05)\n",
        "original_image = noise_scheduler.alphas_cumprod.cpu() ** 0.5\n",
        "noise = (1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5\n",
        "\n",
        "plt.title(\"High values for beta_end\")\n",
        "plt.plot(original_image, label=\"Component of original image\")\n",
        "plt.plot(noise, label=\"Component of noise\")\n",
        "plt.legend(fontsize=\"medium\", loc='center right')\n",
        "\n",
        "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
        "noise = torch.randn_like(batch)\n",
        "noisy_batch = noise_scheduler.add_noise(batch, noise, timesteps)\n",
        "print(\"Noisy X shape\", noisy_batch.shape)\n",
        "show_images(noisy_batch).resize((8 * 64, 64), resample=Image.NEAREST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZCt9iXbprnZx"
      },
      "outputs": [],
      "source": [
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\")\n",
        "original_image = noise_scheduler.alphas_cumprod.cpu() ** 0.5\n",
        "noise = (1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5\n",
        "\n",
        "plt.title(\"Cosine scheduler\")\n",
        "plt.plot(original_image, label=\"Component of original image\")\n",
        "plt.plot(noise, label=\"Component of noise\")\n",
        "plt.legend(fontsize=\"medium\", loc='center right')\n",
        "\n",
        "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
        "noise = torch.randn_like(batch)\n",
        "noisy_batch = noise_scheduler.add_noise(batch, noise, timesteps)\n",
        "print(\"Noisy X shape\", noisy_batch.shape)\n",
        "show_images(noisy_batch).resize((8 * 64, 64), resample=Image.NEAREST)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mSMZUoUtrnU0"
      },
      "outputs": [],
      "source": [
        "#back to default\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xT9CzEKVkXiw"
      },
      "source": [
        "## U-Net Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
        "\n",
        "image_size = 32\n",
        "batch_size = 64\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),  # Resize\n",
        "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
        "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")\n",
        "\n",
        "batch = next(iter(train_dataloader))[\"images\"].to(device)[:8]\n",
        "#print(\"X shape:\", batch.shape)\n",
        "#show_images(batch).resize((8 * 64, 64), resample=Image.NEAREST)\n",
        "\n",
        "noise_scheduler = DDPMScheduler(num_train_timesteps=1000)\n",
        "original_image = noise_scheduler.alphas_cumprod.cpu() ** 0.5\n",
        "noise = (1 - noise_scheduler.alphas_cumprod.cpu()) ** 0.5\n",
        "timesteps = torch.linspace(0, 999, 8).long().to(device)\n",
        "noise = torch.randn_like(batch)\n",
        "noisy_batch = noise_scheduler.add_noise(batch, noise, timesteps)"
      ],
      "metadata": {
        "id": "QK8tivuw5Z7c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WclsG8XernSO"
      },
      "outputs": [],
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "    ),\n",
        ")\n",
        "model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Csqm_UzKrnN8"
      },
      "outputs": [],
      "source": [
        "noisy_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gef87AMWrnKh"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    model_prediction = model(noisy_batch, timesteps).sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvBeuhNRrnGh"
      },
      "outputs": [],
      "source": [
        "assert noisy_batch.shape == model_prediction.shape\n",
        "print(f\"Images are the same shape: {model_prediction.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZQ3gc3svMT3"
      },
      "source": [
        "## Train a model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "3R0JF2GTDR5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "    ),\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "sBdbkukmDYfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
        "\n",
        "image_size = 32 # smaller to reduce training time\n",
        "batch_size = 64\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),  # Resize\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
        "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
        "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "lU_pBxGPFOOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rWKP1j9KwmzC"
      },
      "outputs": [],
      "source": [
        "#build up to the for loop\n",
        "noise_scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000, beta_schedule=\"squaredcos_cap_v2\"\n",
        ")\n",
        "\n",
        "batch = next(iter(train_dataloader))\n",
        "print(batch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yT-tzxA_yGIc"
      },
      "outputs": [],
      "source": [
        "batch[\"images\"].shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLl0ZzocyLOW"
      },
      "outputs": [],
      "source": [
        "clean_images = batch[\"images\"].to(device)\n",
        "clean_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woTWwS5M4-8F"
      },
      "outputs": [],
      "source": [
        "noise = torch.randn(clean_images.shape).to(device)\n",
        "noise.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAqq7z1pyOQu"
      },
      "outputs": [],
      "source": [
        "batch_size = clean_images.shape[0]\n",
        "timesteps = torch.randint(\n",
        "            low=0, high=noise_scheduler.num_train_timesteps, size=(batch_size,), device=device).long()\n",
        "timesteps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTvod4F03pP7"
      },
      "outputs": [],
      "source": [
        "noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "noisy_images.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEQzvXUl6kH3"
      },
      "outputs": [],
      "source": [
        "lr = 1e-4\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0QpDaUeArnDr"
      },
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "        # Sample noise to add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(device)\n",
        "        bs = clean_images.shape[0]\n",
        "\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bs,), device=device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        loss.backward(loss)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
        "        print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoHhwQJjH14n"
      },
      "outputs": [],
      "source": [
        "image_pipeline = DDPMPipeline(unet=model, scheduler=noise_scheduler)\n",
        "sample_image = image_pipeline()\n",
        "sample_image.images[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6CkpNnSrm1i"
      },
      "outputs": [],
      "source": [
        "image_pipeline.save_pretrained(\"DDPM_pipeline\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4PEfp3Q3rmzt"
      },
      "outputs": [],
      "source": [
        "!ls -la DDPM_pipeline/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DDPM_pipeline/model_index.json"
      ],
      "metadata": {
        "id": "cSVKxdmV82f_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9TlnovIrmw0"
      },
      "outputs": [],
      "source": [
        "!ls -la DDPM_pipeline/unet"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat DDPM_pipeline/unet/config.json"
      ],
      "metadata": {
        "id": "ORGKpi_vDkZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YT2E4udormuK"
      },
      "outputs": [],
      "source": [
        "!ls -la DDPM_pipeline/scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "461iPlLCrmrs"
      },
      "outputs": [],
      "source": [
        "!cat DDPM_pipeline/scheduler/scheduler_config.json"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "xFdPZUXtT2_m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- try another dataset? else butterfly with image sizes 64"
      ],
      "metadata": {
        "id": "A9eFSmj0VW3X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "K0eKQMPUUQpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"huggan/smithsonian_butterflies_subset\", split=\"train\")\n",
        "\n",
        "image_size = 64 # smaller to reduce training time\n",
        "batch_size = 64\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.Resize((image_size, image_size)),  # Resize\n",
        "        transforms.RandomHorizontalFlip(),  # Randomly flip (data augmentation)\n",
        "        transforms.ToTensor(),  # Convert to tensor (0, 1)\n",
        "        transforms.Normalize([0.5], [0.5]),  # Map to (-1, 1)\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image.convert(\"RGB\")) for image in examples[\"image\"]]\n",
        "    return {\"images\": images}\n",
        "\n",
        "\n",
        "dataset.set_transform(transform)\n",
        "\n",
        "# Create a dataloader from the dataset to serve up the transformed images in batches\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    dataset, batch_size=batch_size, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "1sg-HtBHT37S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from diffusers import UNet2DModel\n",
        "\n",
        "model = UNet2DModel(\n",
        "    sample_size=image_size,  # the target image resolution\n",
        "    in_channels=3,  # the number of input channels, 3 for RGB images\n",
        "    out_channels=3,  # the number of output channels\n",
        "    layers_per_block=2,  # how many ResNet layers to use per UNet block\n",
        "    block_out_channels=(64, 128, 128, 256),  # More channels -> more parameters\n",
        "    down_block_types=(\n",
        "        \"DownBlock2D\",  # a regular ResNet downsampling block\n",
        "        \"DownBlock2D\",\n",
        "        \"AttnDownBlock2D\",  # a ResNet downsampling block with spatial self-attention\n",
        "        \"AttnDownBlock2D\",\n",
        "    ),\n",
        "    up_block_types=(\n",
        "        \"AttnUpBlock2D\",\n",
        "        \"AttnUpBlock2D\",  # a ResNet upsampling block with spatial self-attention\n",
        "        \"UpBlock2D\",\n",
        "        \"UpBlock2D\",  # a regular ResNet upsampling block\n",
        "    ),\n",
        ")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "f9ZgE0jpUQpl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 20\n",
        "losses = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "        # Sample noise to add to the images\n",
        "        noise = torch.randn(clean_images.shape).to(device)\n",
        "        bs = clean_images.shape[0]\n",
        "\n",
        "        # Sample a random timestep for each image\n",
        "        timesteps = torch.randint(\n",
        "            0, noise_scheduler.num_train_timesteps, (bs,), device=device\n",
        "        ).long()\n",
        "\n",
        "        # Add noise to the clean images according to the noise magnitude at each timestep\n",
        "        noisy_images = noise_scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        noise_pred = model(noisy_images, timesteps, return_dict=False)[0]\n",
        "\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "        loss.backward(loss)\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        loss_last_epoch = sum(losses[-len(train_dataloader) :]) / len(train_dataloader)\n",
        "        print(f\"Epoch:{epoch+1}, loss: {loss_last_epoch}\")"
      ],
      "metadata": {
        "id": "9952ar_5UBWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Challenge"
      ],
      "metadata": {
        "id": "MY4ltfrcP5Pu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- CIFAR-10: https://www.cs.toronto.edu/~kriz/cifar.html"
      ],
      "metadata": {
        "id": "59IMayepQCzP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "cifar10 = load_dataset(\"cifar10\")"
      ],
      "metadata": {
        "id": "PGIaJ0hBP754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M2ZKm2yfPqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FsHWp57zZZJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Solution"
      ],
      "metadata": {
        "id": "ueQuBd84VfEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "NwLU0j5KVjMa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "cifar10 = load_dataset(\"cifar10\")\n",
        "\n",
        "preprocess = transforms.Compose(\n",
        "    [\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "#        transforms.Pad(2),\n",
        "        transforms.Normalize([0.5], [0.5]),\n",
        "    ]\n",
        ")\n",
        "\n",
        "\n",
        "def transform(examples):\n",
        "    images = [preprocess(image) for image in examples[\"img\"]]\n",
        "    return {\"images\": images, \"labels\": examples[\"label\"]}\n",
        "\n",
        "\n",
        "train_dataset = cifar10[\"train\"].with_transform(transform)\n",
        "\n",
        "train_dataloader = torch.utils.data.DataLoader(\n",
        "    train_dataset, batch_size=256, shuffle=True\n",
        ")"
      ],
      "metadata": {
        "id": "TuRs-k3tw-1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cifar10"
      ],
      "metadata": {
        "id": "jWbNmMlXaJg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_size = 32\n",
        "\n",
        "model = UNet2DModel(\n",
        "    in_channels=3,  # 1 channel for grayscale images\n",
        "    out_channels=3,\n",
        "    sample_size=32,\n",
        "    block_out_channels=(32, 64, 128, 256),\n",
        "    num_class_embeds=10,  # Enable class conditioning\n",
        ")\n"
      ],
      "metadata": {
        "id": "tSXvyNZXzm8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02\n",
        ")\n",
        "timesteps = torch.linspace(0, 999, 8).long()\n",
        "batch = next(iter(train_dataloader))\n",
        "x = batch[\"images\"][0].expand([8, 3, sample_size, sample_size])\n",
        "noise = torch.rand_like(x)\n",
        "noised_x = scheduler.add_noise(x, noise, timesteps)\n",
        "show_images((noised_x * 0.5 + 0.5).clip(0, 1))"
      ],
      "metadata": {
        "id": "dNjmCk-9z5SE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn import functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "num_epochs = 3\n",
        "lr = 3e-4\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = model.to(device)\n",
        "scheduler = DDPMScheduler(\n",
        "    num_train_timesteps=1000, beta_start=0.0001, beta_end=0.02\n",
        ")\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=lr, eps=1e-5)\n",
        "losses = []\n",
        "\n",
        "for epoch in (tqdm(range(num_epochs))):\n",
        "    for step, batch in (inner := tqdm(enumerate(train_dataloader), position=0, leave=True, total=len(train_dataloader))):\n",
        "\n",
        "        clean_images = batch[\"images\"].to(device)\n",
        "        class_labels = batch[\"labels\"].to(device)\n",
        "        batch_size = clean_images.shape[0]\n",
        "\n",
        "        noise = torch.randn(clean_images.shape).to(device)\n",
        "\n",
        "        timesteps = torch.randint(\n",
        "            0, scheduler.config.num_train_timesteps, (batch_size,), device=device\n",
        "        ).long()\n",
        "\n",
        "        noisy_images = scheduler.add_noise(clean_images, noise, timesteps)\n",
        "\n",
        "        noise_pred = model(noisy_images, timesteps, class_labels=class_labels, return_dict=False)[0]\n",
        "\n",
        "        loss = F.mse_loss(noise_pred, noise)\n",
        "\n",
        "        inner.set_postfix(loss=f\"{loss.cpu().item():.3f}\")\n",
        "\n",
        "        losses.append(loss.item())\n",
        "\n",
        "        loss.backward(loss)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()"
      ],
      "metadata": {
        "id": "yenoQu4rzGZt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_from_dataset(class_to_generate=0, n_samples=8):\n",
        "    sample = torch.randn(n_samples, 3, 32, 32).to(device)\n",
        "    class_labels = [class_to_generate] * n_samples\n",
        "    class_labels = torch.tensor(class_labels).to(device)\n",
        "\n",
        "    for _, t in tqdm(enumerate(scheduler.timesteps)):\n",
        "        with torch.no_grad():\n",
        "            noise_pred = model(sample, t, class_labels=class_labels).sample\n",
        "\n",
        "        sample = scheduler.step(noise_pred, t, sample).prev_sample\n",
        "\n",
        "    return sample.clip(-1, 1) * 0.5 + 0.5\n",
        "\n",
        "images = generate_from_dataset(class_to_generate=6)\n",
        "show_images(images)"
      ],
      "metadata": {
        "id": "5IoFX1Mi1mJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Label | Description |\n",
        "|-------|-------------|\n",
        "| 0     | airplane    |\n",
        "| 1     | automobile  |\n",
        "| 2     | bird        |\n",
        "| 3     | cat         |\n",
        "| 4     | deer        |\n",
        "| 5     | dog         |\n",
        "| 6     | frog        |\n",
        "| 7     | horse       |\n",
        "| 8     | ship        |\n",
        "| 9     | truck       |\n"
      ],
      "metadata": {
        "id": "csEbghIcJqSV"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9fJlGSB0N7V3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Making improvements - Latent Diffusion"
      ],
      "metadata": {
        "id": "J3vbzFlqdvP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "from diffusers import AutoencoderKL, StableDiffusionPipeline\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import requests\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "lI2ofO782DrS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/jonfernandes/images/raw/main/boat.png\"\n",
        "\n",
        "def download_image(url):\n",
        "  return Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "input_image = download_image(url).resize((512, 512))\n",
        "input_image"
      ],
      "metadata": {
        "id": "O_k0gVPqjgq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJnOD7ekxqeS"
      },
      "source": [
        "- We want to go from image to latent so 3x512x512 to 1x4x64x64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zVMDQ9awtFLI"
      },
      "outputs": [],
      "source": [
        "transforms.ToTensor()(input_image).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rtjF4qyWvi_N"
      },
      "outputs": [],
      "source": [
        "transforms.ToTensor()(input_image).unsqueeze(0).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jMp8CB8cvumZ"
      },
      "outputs": [],
      "source": [
        "four_channels = transforms.ToTensor()(input_image).unsqueeze(0).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\").to(device)\n",
        "vae.config"
      ],
      "metadata": {
        "id": "lnH5c1ZNmZsI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def image_to_latent(input_im):\n",
        "    with torch.no_grad():\n",
        "        latent = vae.encode(transforms.ToTensor()(input_im).unsqueeze(0).to(device)*2-1)\n",
        "    return 0.18215 * latent.latent_dist.sample()\n",
        "\n",
        "encoded = image_to_latent(input_image)\n",
        "encoded.shape"
      ],
      "metadata": {
        "id": "sRAUtH0OpA4X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ3GO94ij6K9"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 4, figsize=(16, 4))\n",
        "for c in range(4):\n",
        "    axs[c].imshow(encoded[0][c].cpu(), cmap='Greys')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8oI29Gij6IU"
      },
      "outputs": [],
      "source": [
        "def latents_to_images(latents):\n",
        "    latents = (1 / 0.18215) * latents\n",
        "    with torch.no_grad():\n",
        "        image = vae.decode(latents).sample\n",
        "    image = (image / 2 + 0.5).clamp(0, 1)\n",
        "    image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
        "    images = (image * 255).round().astype(\"uint8\")\n",
        "    all_images = [Image.fromarray(image) for image in images]\n",
        "    return all_images\n",
        "\n",
        "print(f\"Latents dimension: {encoded.shape}\")\n",
        "latents_to_images(encoded)[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6y93Xrqj6Fs"
      },
      "outputs": [],
      "source": [
        "#original image\n",
        "input_image"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text encoder - CLIP Model"
      ],
      "metadata": {
        "id": "3OLpCIli0XRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "from diffusers import AutoencoderKL, StableDiffusionPipeline\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import requests\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPModel, CLIPProcessor\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "yugTrR79ofNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://github.com/jonfernandes/images/raw/main/boat.png\"\n",
        "\n",
        "def download_image(url):\n",
        "  return Image.open(requests.get(url, stream=True).raw).convert(\"RGB\")\n",
        "\n",
        "input_image = download_image(url).resize((512, 512))\n",
        "input_image"
      ],
      "metadata": {
        "id": "RJMI-X31qNbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"openai/clip-vit-large-patch14\"\n",
        "\n",
        "model = CLIPModel.from_pretrained(model_id)\n",
        "tokenizer = CLIPProcessor.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "TMQec8T-0aTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "photos = [\"a photo of a boat\", \"a photo of a dolphin\"]\n",
        "inp = tokenizer([\"a photo of a boat\", \"a photo of a dolphin\"], images=input_image, return_tensors=\"pt\")\n",
        "inp"
      ],
      "metadata": {
        "id": "vSq9eKXY19sN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outp = model(**inp)\n",
        "outp"
      ],
      "metadata": {
        "id": "Q1nCnEHE2W6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outp.logits_per_image"
      ],
      "metadata": {
        "id": "7Ytia9JU2bpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.nn.functional.softmax(outp.logits_per_image, dim=-1)"
      ],
      "metadata": {
        "id": "vO29GyiP2dA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "photo = [\"a photo of a boat\", \"a photo of a dolphin\"]"
      ],
      "metadata": {
        "id": "_gmVG5lF9jis"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tjabgnVq_aaT"
      },
      "outputs": [],
      "source": [
        "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.vocab_size"
      ],
      "metadata": {
        "id": "CGf8fxtV8wC8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pkH4BT3jfoob"
      },
      "outputs": [],
      "source": [
        "tokenizer.get_vocab()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A62deB5DmtZ"
      },
      "outputs": [],
      "source": [
        "tokenizer.special_tokens_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7XjjtrIoIEQA"
      },
      "outputs": [],
      "source": [
        "tokenizer.bos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jyjle62nIIZe"
      },
      "outputs": [],
      "source": [
        "tokenizer.eos_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mCBIXjkIMi6"
      },
      "outputs": [],
      "source": [
        "tokenizer.pad_token_id"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IQONr9tyIW1x"
      },
      "outputs": [],
      "source": [
        "prompt = \"boat on the sea\"\n",
        "inp = tokenizer(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EIMHpu9zIi1n"
      },
      "outputs": [],
      "source": [
        "tokenizer.convert_ids_to_tokens(inp[\"input_ids\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inp"
      ],
      "metadata": {
        "id": "Nmpcc8TM4sHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BbV8ubKRVmgh"
      },
      "outputs": [],
      "source": [
        "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\").to(device)\n",
        "text_encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UZq98bsVYIVV"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ErYxwGIzZ7AT"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings.token_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QSfFkH9BbHHy"
      },
      "outputs": [],
      "source": [
        "tokenizer(\"boat\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwbG2KSVau7k"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings.token_embedding(torch.tensor(4440, device=device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrOpHrFUa1xI"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings.token_embedding(torch.tensor(4440, device=device)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9FsSxIxa9Qz"
      },
      "outputs": [],
      "source": [
        "text_encoder.text_model.embeddings.token_embedding(torch.tensor(inp[\"input_ids\"], device=device)).shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wluQy3hEeUQo"
      },
      "outputs": [],
      "source": [
        "# 77 is the maximum number of tokens in the text input\n",
        "text_encoder.text_model.embeddings.position_embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Putting the components together using Stable Diffusion"
      ],
      "metadata": {
        "id": "CX8lGQXPGjzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qq -U datasets==2.16.0 transformers==4.36.0 accelerate==0.25.0 ftfy==6.1.3 pyarrow==14.0.0\n",
        "!pip install diffusers[training]==0.25.0\n",
        "!pip install diffusers==0.25.0\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from diffusers import DDPMPipeline, DDIMScheduler, UNet2DModel, DDPMScheduler\n",
        "from diffusers import AutoencoderKL, StableDiffusionPipeline\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import requests\n",
        "from transformers import CLIPTokenizer, CLIPTextModel, CLIPModel, CLIPProcessor\n",
        "\n",
        "def show_images(x):\n",
        "    \"\"\"Given a batch of images x, make a grid and convert to PIL\"\"\"\n",
        "    x = x * 0.5 + 0.5  # Map from (-1, 1) back to (0, 1)\n",
        "    grid = torchvision.utils.make_grid(x)\n",
        "    grid_im = grid.detach().cpu().permute(1, 2, 0).clip(0, 1) * 255\n",
        "    grid_im = Image.fromarray(np.array(grid_im).astype(np.uint8))\n",
        "    return grid_im\n",
        "\n",
        "def display_sample(sample, i):\n",
        "    image_processed = sample.cpu().permute(0, 2, 3, 1)\n",
        "    image_processed = (image_processed + 1.0) * 127.5\n",
        "    image_processed = image_processed.numpy().astype(np.uint8)\n",
        "\n",
        "    image_pil = PIL.Image.fromarray(image_processed[0])\n",
        "    display(f\"Image at step {i}\")\n",
        "    display(image_pil)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "SXwXKrSdI6w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vae = AutoencoderKL.from_pretrained(\n",
        "    \"stabilityai/sd-vae-ft-ema\", torch_dtype=torch.float16\n",
        ").to(device)\n",
        "pipeline = StableDiffusionPipeline.from_pretrained(\n",
        "    \"runwayml/stable-diffusion-v1-5\",\n",
        "    vae=vae,\n",
        "    torch_dtype=torch.float16,\n",
        "    variant=\"fp16\",\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "A0tbAfdUIr87"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images = []\n",
        "prompt = \"A boat on the sea\"\n",
        "for guidance_scale in [1, 2, 4, 12]:\n",
        "    torch.manual_seed(0)\n",
        "    image = pipeline(prompt, guidance_scale=guidance_scale).images[0]\n",
        "    images.append(image)"
      ],
      "metadata": {
        "id": "fy20mNFJDv2n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig, axs = plt.subplots(1, 4, figsize=(12, 6))\n",
        "\n",
        "for i in range(4):\n",
        "  axs[i].imshow(images[i])\n",
        "  axs[i].axis('off')\n",
        "\n",
        "plt.tight_layout()"
      ],
      "metadata": {
        "id": "GN0lV4oYF_QS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = [\n",
        "    \"A boat on the sea\"\n",
        "]\n",
        "height = 512\n",
        "width = 512\n",
        "num_inference_steps = 30\n",
        "guidance_scale = 7.5\n",
        "seed = 1"
      ],
      "metadata": {
        "id": "J3TDvM7hGlKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_input = pipeline.tokenizer(\n",
        "    prompt,\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipeline.tokenizer.model_max_length,\n",
        "    truncation=True,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "unconditioned_input = pipeline.tokenizer(\n",
        "    \"\",\n",
        "    padding=\"max_length\",\n",
        "    max_length=pipeline.tokenizer.model_max_length,\n",
        "    return_tensors=\"pt\",\n",
        ")\n",
        "\n",
        "with torch.no_grad():\n",
        "    text_embeddings = pipeline.text_encoder(text_input.input_ids.to(device))[0]\n",
        "    unconditioned_embeddings = pipeline.text_encoder(unconditioned_input.input_ids.to(device))[0]\n",
        "\n",
        "text_embeddings = torch.cat([unconditioned_embeddings, text_embeddings])"
      ],
      "metadata": {
        "id": "tZAoSyXuGrXJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline.scheduler.set_timesteps(num_inference_steps)\n",
        "\n",
        "latents = (\n",
        "    torch.randn((1, pipeline.unet.config.in_channels, height // 8, width // 8)).to(device).half()\n",
        ")\n",
        "latents = latents * pipeline.scheduler.init_noise_sigma"
      ],
      "metadata": {
        "id": "dPT76aDIG36R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, timestep in enumerate(pipeline.scheduler.timesteps):\n",
        "    latent_model_input = torch.cat([latents] * 2)\n",
        "    latent_model_input = pipeline.scheduler.scale_model_input(latent_model_input, timestep)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        noise_pred = pipeline.unet(\n",
        "            latent_model_input, timestep, encoder_hidden_states=text_embeddings).sample\n",
        "\n",
        "    noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
        "\n",
        "    noise_pred = noise_pred_uncond + guidance_scale * (\n",
        "        noise_pred_text - noise_pred_uncond\n",
        "    )\n",
        "\n",
        "    latents = pipeline.scheduler.step(noise_pred, timestep, latents).prev_sample"
      ],
      "metadata": {
        "id": "xYlwRTymG4wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "latents = 1 / vae.config.scaling_factor * latents\n",
        "with torch.no_grad():\n",
        "    image = vae.decode(latents).sample\n",
        "image = (image / 2 + 0.5).clamp(0, 1)\n",
        "\n",
        "show_images(image[0])"
      ],
      "metadata": {
        "id": "wJnzDHYzHCT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sy7VUXg9HCs-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}